{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYgtGOfvj67dnsL9IXR92h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LWz_MukvDnT",
        "outputId": "7c076b9c-4418-4aeb-f9d1-ca9acf346fbd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Mounting Google Drive... Please authorize.\")\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXCIA-XvvLTB",
        "outputId": "202ae2a5-b084-463a-b07e-7d4cd54fe741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive... Please authorize.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Model Path and Load Model\n",
        "\n",
        "# This MUST match the 'output_dir' you used for training\n",
        "MODEL_PATH = \"/content/drive/MyDrive/final-punctuation-model/checkpoint-31074\"\n",
        "\n",
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "\n",
        "try:\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    # 2. Setup Device and Eval Mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Model loaded successfully and moved to device:\", device)\n",
        "\n",
        "except OSError:\n",
        "    print(f\"Error: Could not find model at {MODEL_PATH}\")\n",
        "    print(\"Please make sure the path is correct and training was completed.\")\n",
        "\n",
        "# 3. Define the Prediction Function\n",
        "\n",
        "def correct(text):\n",
        "    \"\"\"\n",
        "    Takes a raw string, preprocesses it, and returns the\n",
        "    corrected text using the fine-tuned T5 model.\n",
        "    \"\"\"\n",
        "    if 'model' not in globals():\n",
        "        print(\"Model is not loaded. Cannot run correction.\")\n",
        "        return None\n",
        "\n",
        "    # Preprocess: Add prefix, lowercase, and remove punctuation\n",
        "    input_text = \"correct: \" + re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,  # Must match the training max_length\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate the corrected text\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=512,       # Max length of the *output*\n",
        "            num_beams=4,          # Use beam search for better results\n",
        "            early_stopping=True   # Stop when the <eos> token is generated\n",
        "        )\n",
        "\n",
        "    # Decode the output tokens back into a string\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfYlopBlvOXi",
        "outputId": "fd2d8344-6d1c-42b5-f730-09b7443347e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/final-punctuation-model/checkpoint-31074\n",
            "Model loaded successfully and moved to device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "\n",
        "# We'll re-use the `correct()` function from Cell 3.\n",
        "# Make sure Cell 3 has been run.\n",
        "\n",
        "def correct_large_paragraph(long_text):\n",
        "    \"\"\"\n",
        "    Corrects long text by splitting it into manageable chunks\n",
        "    and correcting each one individually.\n",
        "    \"\"\"\n",
        "    if 'model' not in globals():\n",
        "        print(\"Model is not loaded. Cannot run correction.\")\n",
        "        return None\n",
        "\n",
        "    # 1. Split the *original* text by whitespace\n",
        "    # This is slightly better than doing regex on the whole\n",
        "    # thing at the start.\n",
        "    words = long_text.split()\n",
        "\n",
        "    if not words:\n",
        "        return \"\"\n",
        "\n",
        "    # --- 2. Define a safe chunk size ---\n",
        "    # 128 tokens is roughly ~100 words.\n",
        "    # We use 80 to be safe and give the model room.\n",
        "    chunk_size = 70\n",
        "\n",
        "    corrected_chunks = []\n",
        "    num_chunks = math.ceil(len(words) / chunk_size)\n",
        "\n",
        "    print(f\"Input is long. Splitting into {num_chunks} chunks...\")\n",
        "\n",
        "    # --- 3. Loop through the text in chunks ---\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "\n",
        "        # Get the chunk of words\n",
        "        chunk_of_words = words[i : i + chunk_size]\n",
        "\n",
        "        # Join them back into a single string\n",
        "        text_chunk = \" \".join(chunk_of_words)\n",
        "\n",
        "        print(f\"--- Correcting chunk {i // chunk_size + 1}/{num_chunks} ---\")\n",
        "\n",
        "        # 4. Run our *existing* `correct()` function on the chunk\n",
        "        # This function (from Cell 3) handles the regex and prefix\n",
        "        corrected_chunk = correct(text_chunk)\n",
        "        corrected_chunks.append(corrected_chunk)\n",
        "\n",
        "    # --- 5. Join the corrected chunks back together ---\n",
        "    return \" \".join(corrected_chunks)\n",
        "\n",
        "def check(sentence):\n",
        "  threshold = 70\n",
        "  words = sentence.split()\n",
        "\n",
        "  if len(words) > threshold:\n",
        "    #print(f\"Input is long ({len(words)} words). Using chunking function...\")\n",
        "    return correct_large_paragraph(sentence)\n",
        "  else:\n",
        "    #print(f\"Input is short ({len(words)} words). Using single-pass function...\")\n",
        "    return correct(sentence)"
      ],
      "metadata": {
        "id": "1-zjv72vglmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Rule Based\n",
        "\n",
        "# --- Rule 1: Spacing Fixes ---\n",
        "def refine_spacing(text):\n",
        "    \"\"\"\n",
        "    Cleans up spacing errors around punctuation.\n",
        "    - 'word .' -> 'word.'\n",
        "    - 'word,word' -> 'word, word'\n",
        "    - 'word  word' -> 'word word'\n",
        "    \"\"\"\n",
        "    # Remove space *before* punctuation\n",
        "    text = re.sub(r'\\s+([.,?])', r'\\1', text)\n",
        "\n",
        "    # Ensure space *after* punctuation (if followed by a letter)\n",
        "    text = re.sub(r'([.,?])([a-zA-Z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Collapse multiple spaces into one\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Collapse multiple terminal punctuations\n",
        "    text = re.sub(r'([.?!]){2,}', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# --- Rule 2: Contraction Fixes ---\n",
        "def refine_contractions(text):\n",
        "    \"\"\"\n",
        "    Fixes common contraction errors the model might make.\n",
        "    Uses \\b (word boundary) to avoid changing words like \"itself\".\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\b(its)\\b\", \"it's\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(dont)\\b\", \"don't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(cant)\\b\", \"can't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(wont)\\b\", \"won't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(isnt)\\b\", \"isn't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(arent)\\b\", \"aren't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(wasnt)\\b\", \"wasn't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(werent)\\b\", \"weren't\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(hes)\\b\", \"he's\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(shes)\\b\", \"she's\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(theyre)\\b\", \"they're\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(youre)\\b\", \"you're\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(im)\\b\", \"I'm\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(ive)\\b\", \"I've\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\b(id)\\b\", \"I'd\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# --- Rule 3: \"Lone I\" Fix ---\n",
        "def refine_lone_i(text):\n",
        "    \"\"\"\n",
        "    Capitalizes the lone pronoun 'i' (e.g., \" i \" -> \" I \").\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\b( i )\\b\", \" I \", text)\n",
        "    text = re.sub(r\"\\b( i')\\b\", \" I'\", text)  # For \"i'm\", \"i've\"\n",
        "    return text\n",
        "\n",
        "\n",
        "# --- Rule 4: Capitalization After Sentence End ---\n",
        "def refine_capitalization(text):\n",
        "    \"\"\"\n",
        "    Ensures that the first letter after a terminal punctuation\n",
        "    (. ! ?) is capitalized.\n",
        "    \"\"\"\n",
        "\n",
        "    def capitalize_match(match):\n",
        "        # match.group(1): punctuation and space (e.g., \". \")\n",
        "        # match.group(2): lowercase letter (e.g., \"t\")\n",
        "        return match.group(1) + match.group(2).upper()\n",
        "\n",
        "    # Find a terminal punctuation, followed by whitespace,\n",
        "    # and capture the first lowercase letter.\n",
        "    text = re.sub(r'([.?!]\\s+)([a-z])', capitalize_match, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# -The Main \"Pipeline\" Function\n",
        "def post_process_refinement(model_output_text):\n",
        "    \"\"\"\n",
        "    Runs the model's output through the full pipeline of\n",
        "    rule-based refinement functions.\n",
        "\n",
        "    The order matters:\n",
        "    1. Fix contractions/lone 'i' first.\n",
        "    2. Fix spacing.\n",
        "    3. Fix capitalization (so it can use the new periods).\n",
        "    \"\"\"\n",
        "    if model_output_text:\n",
        "        text = model_output_text[0].upper() + model_output_text[1:]\n",
        "    else:\n",
        "        text = model_output_text\n",
        "\n",
        "    text = refine_contractions(text)\n",
        "    text = refine_lone_i(text)\n",
        "    text = refine_spacing(text)\n",
        "    text = refine_capitalization(text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "5j5vIwD33Sv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Simple\n",
        "text_1 = \"hello my name is shahaan what is yours\"\n",
        "print(f\"Input:    '{text_1}'\")\n",
        "model_out = check(text_1)\n",
        "print(f\"Model Output:   '{model_out}'\")\n",
        "print(f\"Polished Output: '{post_process_refinement(model_out)}'\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# Test 2: Longer Paragraph\n",
        "text_2 = \"the quick brown fox jumps over the lazy dog this is a classic sentence used for typing practice but it also serves as a good test for our model i wonder if it will know where to put the period and how to capitalize the word 'this' in the middle of the text it's a non-trivial task because the model has to understand context not just individual words for example will it know what to do with a sentence like this what do you think the final output will be i am very excited to see the results\"\n",
        "print(f\"Input: '{text_2}'\")\n",
        "model_out = check(text_2)\n",
        "print(f\"Model Output:   '{model_out}'\")\n",
        "print(f\"Polished Output: '{post_process_refinement(model_out)}'\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# Test 3: Your Own Text\n",
        "text_3 = \"type any text you want here i hope this works\"\n",
        "print(f\"Input: '{text_2}'\")\n",
        "model_out = check(text_3)\n",
        "print(f\"Model Output:   '{model_out}'\")\n",
        "print(f\"Polished Output: '{post_process_refinement(model_out)}'\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP-MlfggvRZc",
        "outputId": "2767bb09-dbbb-4ad0-9031-61ab22e8769f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:    'hello my name is shahaan what is yours'\n",
            "Model Output:   'Hello, my name is Shahaan, what is yours?'\n",
            "Polished Output: 'Hello, my name is Shahaan, what is yours?'\n",
            "--------------------\n",
            "Input: 'the quick brown fox jumps over the lazy dog this is a classic sentence used for typing practice but it also serves as a good test for our model i wonder if it will know where to put the period and how to capitalize the word 'this' in the middle of the text it's a non-trivial task because the model has to understand context not just individual words for example will it know what to do with a sentence like this what do you think the final output will be i am very excited to see the results'\n",
            "Input is long. Splitting into 2 chunks...\n",
            "--- Correcting chunk 1/2 ---\n",
            "--- Correcting chunk 2/2 ---\n",
            "Model Output:   'The quick brown fox jumps over the lazy dog. This is a classic sentence used for typing practice, but it also serves as a good test for our model. I wonder if it will know where to put the period and how to capitalize the word. This in the middle of the text, its a nontrivial task because the model has to understand context, not just individual words. For example, Will it know what to do with a sentence like this? What do you think the final output will be? I am very excited to see the results?'\n",
            "Polished Output: 'The quick brown fox jumps over the lazy dog. This is a classic sentence used for typing practice, but it also serves as a good test for our model. I wonder if it will know where to put the period and how to capitalize the word. This in the middle of the text, it's a nontrivial task because the model has to understand context, not just individual words. For example, Will it know what to do with a sentence like this? What do you think the final output will be? I am very excited to see the results?'\n",
            "--------------------\n",
            "Input: 'the quick brown fox jumps over the lazy dog this is a classic sentence used for typing practice but it also serves as a good test for our model i wonder if it will know where to put the period and how to capitalize the word 'this' in the middle of the text it's a non-trivial task because the model has to understand context not just individual words for example will it know what to do with a sentence like this what do you think the final output will be i am very excited to see the results'\n",
            "Model Output:   'Type any text you want here. I hope this works.'\n",
            "Polished Output: 'Type any text you want here. I hope this works.'\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"\"\"The complexity of human language presents! a profound challenge for artificial intelligence It's not merely a structured system of; vocabulary; rather it's a dynamic entity deeply intertwined with? context, culture and subtle intention Natural Language Processing (NLP;) models often built on vast statistical analysis strive to? comprehend and generate text with human-like fluency However a fascinating; inverse problem exists modeling human error Simulating a typo isn't just random substitution it involves understanding keyboard layouts common transpositions, and phonetic similarities More advanced simulations, such as semantic errors require a sophisticated grasp of how the, human mind retrieves and associates words This \"reverse engineering\" of mistakes purposefully? creating plausible incorrectness is? not only a creative exercise but also a powerful method for! building and evaluating more robust, correction systems\"\"\"\n",
        "print(f\"Input:    '{my_text}'\")\n",
        "model_out = check(my_text)\n",
        "print(f\"Model Output:   '{model_out}'\")\n",
        "print(f\"Polished Output: '{post_process_refinement(model_out)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5ZhDGEDwM0h",
        "outputId": "81cf1492-73c8-4d6e-cf50-72d760f5de68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:    'The complexity of human language presents! a profound challenge for artificial intelligence It's not merely a structured system of; vocabulary; rather it's a dynamic entity deeply intertwined with? context, culture and subtle intention Natural Language Processing (NLP;) models often built on vast statistical analysis strive to? comprehend and generate text with human-like fluency However a fascinating; inverse problem exists modeling human error Simulating a typo isn't just random substitution it involves understanding keyboard layouts common transpositions, and phonetic similarities More advanced simulations, such as semantic errors require a sophisticated grasp of how the, human mind retrieves and associates words This \"reverse engineering\" of mistakes purposefully? creating plausible incorrectness is? not only a creative exercise but also a powerful method for! building and evaluating more robust, correction systems'\n",
            "Input is long. Splitting into 2 chunks...\n",
            "--- Correcting chunk 1/2 ---\n",
            "--- Correcting chunk 2/2 ---\n",
            "Model Output:   'The complexity of human language presents a profound challenge for artificial intelligence. Its not merely a structured system of vocabulary, rather its a dynamic entity deeply intertwined with context culture and subtle intention. Natural language processing ( NLP ) models, often built on vast statistical analysis, strive to comprehend and generate text with humanlike fluency. However, a fascinating inverse problem exists, modeling human error. Simulating a typo isnt just random substitution it. involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This reverse engineering of mistakes purposefully creating plausible incorrectness is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.'\n",
            "Polished Output: 'The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary, rather it's a dynamic entity deeply intertwined with context culture and subtle intention. Natural language processing ( NLP ) models, often built on vast statistical analysis, strive to comprehend and generate text with humanlike fluency. However, a fascinating inverse problem exists, modeling human error. Simulating a typo isn't just random substitution it. Involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This reverse engineering of mistakes purposefully creating plausible incorrectness is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def check_similarity(str1, str2):\n",
        "    # fuzz.ratio() calculates the Levenshtein distance similarity\n",
        "    similarity_score = fuzz.ratio(str1, str2)\n",
        "    return similarity_score\n",
        "\n",
        "correct_text = '''The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.'''\n",
        "score = check_similarity(correct_text, post_process_refinement(model_out))\n",
        "\n",
        "print(f\"\\nStr1: {correct_text}\")\n",
        "print(f\"Str2: {post_process_refinement(model_out)}\")\n",
        "print(f\"Similarity Score: {score}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6XUw0oI3D_F",
        "outputId": "c592cac9-e58a-40e1-f7e7-e0b646e8d2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Str1: The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.\n",
            "Str2: The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary, rather it's a dynamic entity deeply intertwined with context culture and subtle intention. Natural language processing ( NLP ) models, often built on vast statistical analysis, strive to comprehend and generate text with humanlike fluency. However, a fascinating inverse problem exists, modeling human error. Simulating a typo isn't just random substitution it. Involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This reverse engineering of mistakes purposefully creating plausible incorrectness is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.\n",
            "Similarity Score: 94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNIeEMI7J2Ts"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}