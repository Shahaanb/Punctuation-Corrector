{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "lO_mtEK6jzbp"
      },
      "outputs": [],
      "source": [
        "# --- 1. Install Libraries ---\n",
        "!pip install transformers datasets sacrebleu torch\n",
        "\n",
        "# --- 2. Import All Dependencies ---\n",
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tut2HR85kK4S"
      },
      "outputs": [],
      "source": [
        "# --- Load Raw Data (Our \"Target\" Text) ---\n",
        "print(\"Loading wikitext dataset...\")\n",
        "full_dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
        "\n",
        "# We'll use 100,000 examples as we discussed.\n",
        "# You can lower this to ~5,000 for a very fast test run.\n",
        "slice_size = 100000\n",
        "dataset_slice = full_dataset.select(range(slice_size))\n",
        "\n",
        "print(f\"\\nCreated a working slice of {len(dataset_slice)} documents.\")\n",
        "print(f\"Example 'good' text: \\n'{dataset_slice[5]['text']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g92V-pm3kOl1"
      },
      "outputs": [],
      "source": [
        "def create_seq2seq_examples(example):\n",
        "    text = example['text'].strip()\n",
        "\n",
        "    # 1. Filter out empty lines and headings\n",
        "    if not text or text.startswith(\"=\") or len(text.split()) < 5:\n",
        "        return {\"input_text\": \"\", \"target_text\": \"\"}\n",
        "\n",
        "    # 2. Define the Target (Y) - The \"good\" text\n",
        "    # This is just the original, correct text.\n",
        "    target_text = text\n",
        "\n",
        "    # 3. Define the Input (X) - The \"bad\" text\n",
        "    # We make it lowercase and remove all punctuation\n",
        "    # We use a simple regex to keep only letters, numbers, and whitespace\n",
        "    broken_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "    # 4. Add the T5 Task Prefix\n",
        "    # This prefix tells the model what \"translation\" task to perform.\n",
        "    input_text = \"correct: \" + broken_text\n",
        "\n",
        "    return {\"input_text\": input_text, \"target_text\": target_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yyufblykSXX"
      },
      "outputs": [],
      "source": [
        "print(\"Applying preprocessing to all examples...\")\n",
        "\n",
        "# Apply our function to every example in the dataset\n",
        "# num_proc=4 uses 4 cores to speed this up.\n",
        "raw_dataset = dataset_slice.map(\n",
        "    create_seq2seq_examples,\n",
        "    num_proc=4,\n",
        "    remove_columns=['text']  # We don't need the original 'text' column anymore\n",
        ")\n",
        "\n",
        "# Filter out the empty examples we created\n",
        "processed_dataset = raw_dataset.filter(lambda x: len(x['input_text']) > 10)\n",
        "\n",
        "print(f\"\\nFinished processing. We have {len(processed_dataset)} valid examples.\")\n",
        "print(\"\\nExample of a training pair:\")\n",
        "print(f\"INPUT (X):  '{processed_dataset[5]['input_text']}'\")\n",
        "print(f\"TARGET (Y): '{processed_dataset[5]['target_text']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwRwIKSHkUsI"
      },
      "outputs": [],
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "model_name = \"t5-small\"\n",
        "\n",
        "print(f\"Loading '{model_name}' tokenizer and model...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"\\nModel loaded and moved to {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gW20mFskXE7"
      },
      "outputs": [],
      "source": [
        "# --- Define Tokenization Function ---\n",
        "\n",
        "# We'll truncate sequences to 128 tokens.\n",
        "# T5 is efficient, but this keeps training fast.\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the \"inputs\" (our broken text)\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"input_text\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Tokenize the \"targets\" (our correct text)\n",
        "    # We use this 'with' block to ensure the tokenizer knows\n",
        "    # it's tokenizing the \"target\" or \"label\" text.\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"target_text\"],\n",
        "            max_length=MAX_LENGTH,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    # Add the tokenized labels to our model inputs\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CF53MCfkg0T"
      },
      "outputs": [],
      "source": [
        "# --- Apply Tokenization and Split ---\n",
        "print(\"Tokenizing all examples...\")\n",
        "\n",
        "# Apply the tokenization function to all our examples\n",
        "tokenized_dataset = processed_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['input_text', 'target_text'] # Not needed anymore\n",
        ")\n",
        "\n",
        "# Split the dataset into 90% train, 10% validation\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "print(\"\\nData is tokenized and split:\")\n",
        "print(f\"Training examples:   {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KlExErukkUy"
      },
      "outputs": [],
      "source": [
        "# --- Mount Google Drive ---\n",
        "print(\"Mounting Google Drive... Please authorize.\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SFcajWwkmzq"
      },
      "outputs": [],
      "source": [
        "# --- Set Up Trainer (Corrected) ---\n",
        "\n",
        "# 1. Import the correct Seq2Seq classes\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "\n",
        "# 3. Define the directory in your Google Drive to save the model\n",
        "output_dir = \"/content/drive/MyDrive/t5-punctuation-model\"\n",
        "\n",
        "# 4. This special collator correctly pads both inputs and labels\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "\n",
        "# 5. *** Use Seq2SeqTrainingArguments ***\n",
        "# This class IS designed for T5 and DOES accept 'predict_with_generate'\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,                     # 3 epochs is a good start\n",
        "    per_device_train_batch_size=8,          # 8 is safe for 't5-small' on a T4 GPU\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,                      # Adds regularization\n",
        "\n",
        "    # Evaluation and Saving\n",
        "    eval_strategy=\"epoch\",                # Run validation every epoch\n",
        "    save_strategy=\"epoch\",                # Save a checkpoint every epoch\n",
        "    load_best_model_at_end=True,          # Keep only the best model\n",
        "\n",
        "    # This is the critical argument, and it works with this class\n",
        "    predict_with_generate=True,\n",
        "\n",
        "    report_to=\"none\"                        # Disables online logging\n",
        ")\n",
        "\n",
        "# 6. *** Use Seq2SeqTrainer ***\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\nTrainer initialized successfully with Seq2SeqTrainer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvQfFSObkpjc"
      },
      "outputs": [],
      "source": [
        "# --- Train the Model! ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(f\"The best model has been saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLwPEnIAs2ZX"
      },
      "outputs": [],
      "source": [
        "local_save_path = \"./my-local-t5-model\"\n",
        "trainer.save_model(local_save_path)\n",
        "print(f\"A temporary local copy has also been saved to: {local_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAZEiIvakrYU"
      },
      "outputs": [],
      "source": [
        "# --- Test Your Trained Model ---\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# 1. Load your saved model from Google Drive\n",
        "# The Trainer saves the best model in the 'output_dir'\n",
        "model_path = \"/content/drive/MyDrive/t5-punctuation-model/checkpoint-7818\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# 2. Make sure model is on the GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"Loaded fine-tuned model from Google Drive.\")\n",
        "\n",
        "# 3. Create the prediction function\n",
        "def correct(text):\n",
        "    # Add the \"correct:\" prefix, lowercase, and remove punctuation\n",
        "    input_text = \"correct: \" + re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    # 4. Generate the corrected text\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # 5. Decode the output and return it\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# --- Let's try it! ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"--- TESTING THE NEW S2S MODEL ---\")\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "text_1 = \"hello my name is shahaan what is yours\"\n",
        "text_2 = \"this is a test of the punctuation model i hope it works\"\n",
        "text_3 = \"the game was fun but i think it could be better\"\n",
        "\n",
        "# Test 1\n",
        "print(f\"Input:    '{text_1}'\")\n",
        "print(f\"Output:   '{correct(text_1)}'\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Test 2\n",
        "print(f\"Input:    '{text_2}'\")\n",
        "print(f\"Output:   '{correct(text_2)}'\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Test 3\n",
        "print(f\"Input:    '{text_3}'\")\n",
        "print(f\"Output:   '{correct(text_3)}'\")\n",
        "print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFHL8ZfLuM9P"
      },
      "outputs": [],
      "source": [
        "test_string = \"the quick brown fox jumps over the lazy dog this is a classic sentence used for typing practice but it also serves as a good test for our model i wonder if it will know where to put the period and how to capitalize the word 'this' in the middle of the text it's a non-trivial task because the model has to understand context not just individual words for example will it know what to do with a sentence like this what do you think the final output will be i am excited to see the result\"\n",
        "\n",
        "# Now you can run your function\n",
        "corrected_version = correct(test_string)\n",
        "\n",
        "print(f\"Input:    '{test_string}'\")\n",
        "print(f\"Output:   '{corrected_version}'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNeAHtgIJV1LRRCHMkFmZF7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}